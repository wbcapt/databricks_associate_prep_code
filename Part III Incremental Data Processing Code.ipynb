{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART III Prep\n",
    "\n",
    "https://docs.databricks.com/en/sql/language-manual/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to use Hive Variables to substitute in string values derived from the account email of the current user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "SELECT \"${da.db_name}\" AS db_name,\n",
    "       \"${da.paths.working_dir}\" AS working_dir\n",
    "\n",
    "\n",
    "CREATE DATABASE IF NOT EXISTS ${da.db_name}_default_location;\n",
    "CREATE DATABASE IF NOT EXISTS ${da.db_name}_custom_location LOCATION '${da.paths.working_dir}/_custom_location.db';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify where Delta Lake provides ACID transactions\n",
    "\n",
    "Merge update does this AND Delta Lake guarantees that any read against a table will return the most recent version of the table. The purpose of merge is to consolidate update, insert, and other manipulations into one command much like an upsert\n",
    "\n",
    "DE2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "MERGE INTO students b\n",
    "USING updates u\n",
    "ON b.id=u.id\n",
    "WHEN MATCHED AND u.type = \"update\"\n",
    "  THEN UPDATE SET *\n",
    "WHEN MATCHED AND u.type = \"delete\"\n",
    "  THEN DELETE\n",
    "WHEN NOT MATCHED AND u.type = \"insert\"\n",
    "  THEN INSERT *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Managed vs. External\n",
    "\n",
    "Managed = delta by default, in unity catalog, better performance, meta & data management by databricks, ACID, dropping deletes meta and data\n",
    "\n",
    "External = outside of unity catalog in a specified cloud object store. Can make delta, meta management only, not ACID, dropping deletes only meta the underlying table data will still exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a managed table (Delta by default as it is backed by delta lake) Metadata + Data Management\n",
    "\n",
    "Managed tables manage underlying data files alongside the metastore registration. Databricks recommends that you use managed tables whenever you create a new table. Unity Catalog managed tables are the default when you create tables in Databricks. They always use Delta Lake. See Work with managed tables.\n",
    "\n",
    "By default, managed tables in a database without the location specified will be created in the dbfs:/user/hive/warehouse/<database_name>.db/ directory.\n",
    "\n",
    "We can see that, as expected, the data and metadata for our Delta Table are stored in that location.\n",
    "\n",
    "Dropping = drop metadata and data. Directory is still kept\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "/*full specification and Unity Catalog Approach*/\n",
    "CREATE TABLE catalog_name.schema_name.table_name\n",
    "  (width INT, length INT, height INT);\n",
    "\n",
    "INSERT INTO managed_default\n",
    "VALUES (3 INT, 2 INT, 1 INT)\n",
    "\n",
    "/*HIVE approach, this will create the table in the default location under dbfs:/user/hive/warehouse/ remember the variables up top*/\n",
    "CREATE DATABASE IF NOT EXISTS ${da.db_name}_default_location;\n",
    "CREATE DATABASE IF NOT EXISTS ${da.db_name}_custom_location LOCATION '${da.paths.working_dir}/_custom_location.db';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an External Table (Metadata managed by Databricks only)\n",
    "\n",
    "Here you are creating a delta table in cloud object store\n",
    "\n",
    "Why use external?\n",
    "1. Cost management\n",
    "2. Controlling access and keeping data in one region\n",
    "3. Multiple applications need to access the data and to avoid data duplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "/*Exammple 1 with HIVE*/\n",
    "CREATE TABLE my_external_table\n",
    "  (width INT, length INT, height INT)\n",
    "LOCATION 'dbfs:/mnt/demo/external_default';\n",
    "\n",
    "  \n",
    "INSERT INTO external_default\n",
    "VALUES (3 INT, 2 INT, 1 INT)\n",
    "\n",
    "hive_metastore.default.managed_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "/*Example 2 with ADLS external location*/\n",
    "\n",
    "CREATE TABLE my_external_table\n",
    "    (width INT. length INT, height INT)\n",
    "LOCATION 'abfss://<bucket-path>/<table-directory>';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore table Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "/*basic metadata*/\n",
    "DESCRIBE TABLE customer;\n",
    "\n",
    "DESCRIBE TABLE extended my_table_name;\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "source": [
    "Return information about a schema: \n",
    "\n",
    "Returns information about schema, partitioning, table size, and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "/*Managed Table*/\n",
    "DESCRIBE DETAIL [schema_name].table_name\n",
    "\n",
    "DESCRIBE EXTENDED students_table\n",
    "\n",
    "/*External Location*/\n",
    "DESCRIBE EXTERNAL LOCATION table_location_name;\n",
    "\n",
    "/*Show external locations*/\n",
    "SHOW EXTERNAL LOCATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the location of a table\n",
    "\n",
    "**The exact same for external or Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "/*for External*/\n",
    "DESCRIBE DETAIL my_table\n",
    "\n",
    "/*For Delta*/\n",
    "DESCRIBE DETAIL delta_table_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the SHOW table command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "USE <your_database_name>\n",
    "\n",
    "/*show tables*/\n",
    "SHOW TABLES\n",
    "\n",
    "/*show tables in global temp*/\n",
    "SHOW TABLES IN global_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the directory structure of Delta Lake files\n",
    "\n",
    "DE2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(f\"{DA.paths.user_db}/students\"))\n",
    "\n",
    "#using DB Utils\n",
    "dbuitls.fs.ls('dbfs:/user/hive/warehouse/employees')\n",
    "\n",
    "dbutils.fs.ls('dbfs:/user/hive/warehouse/buster_schema_defult_location.db/my_delta_table')\n",
    "\n",
    "#specific delta log file\n",
    "dbutils.fs.ls('dbfs:/user/hive/warehouse/buster_schema_defult_location.db/my_delta_table/_delta_log')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reasoning about data files\n",
    "\n",
    "DESCRIBE DETAIL allows us to see some other details about our Delta table, including the number of files.\n",
    "\n",
    "DE2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "DESCRIBE DETAIL my_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify who has written previous versions of a table\n",
    "\n",
    "DE2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "DESCRIBE HISTORY table_name;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review a history of table transactions\n",
    "\n",
    "DE2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "DESCRIBE HISTORY my_table_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roll back a table to a previous version\n",
    "\n",
    "DE2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "/*Option 1*/\n",
    "RESTORE TABLE employees TO VERSION AS OF 5\n",
    "\n",
    "/*Option 2*/\n",
    "RESTORE TABLE table_name TO TIMESTAMP AS OF 'yyyy-MM-dd HH:mm:ss';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify that a table can be rolled back to a previous version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "DESCRIBE HISTORY table_name;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query a specific version of a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "SELECT * \n",
    "FROM employees VERSION AS OF 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify why Zordering is beneficial to Delta Lake tables.\n",
    "\n",
    "It helps consolidate the underlying parquet data files for the table that are too small into a more manageable size using criteria specified such as a field. Used in conjunction with Optimize\n",
    "\n",
    "See 1.3 in my_free_trial_databricks & DE 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "OPTIMIZE employees\n",
    "ZORDER BY id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VACUUM command for a table (including override)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "VACUUM my_table\n",
    "\n",
    "/*If you want to override*/\n",
    "SET spark.databricks.delta.retentionDurationCheck.enabled = false;\n",
    "SET spark.databricks.delta.vacuum.logging.enabled = true;\n",
    "\n",
    "VACUUM my_table RETAIN 0 HOURS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify CTAS as a solution and create a table using CTAS\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "CTAS statements automatically infer schema information from query results and do not support manual schema declaration.\n",
    "\n",
    "This means that CTAS statements are useful for external data ingestion from sources with well-defined schema, such as Parquet files and tables.\n",
    "\n",
    "CTAS statements also do not support specifying additional file options.\n",
    "\n",
    "We can see how this would present significant limitations when trying to ingest data from CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "/*FROM A TABLE*/\n",
    "CREATE OR REPLACE TABLE my_table AS\n",
    "SELECT * FROM my_other_table\n",
    "\n",
    "\n",
    "/*FROM A FILE*/\n",
    "CREATE OR REPLACE TABLE sales_unparsed AS\n",
    "SELECT * FROM csv.`${da.paths.datasets}/raw/sales-csv/`;\n",
    "SELECT * FROM sales_unparsed;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a generated column\n",
    "\n",
    "Purpose of generated columns is to do a calculation\n",
    "\n",
    "DE 4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE purchase_dates (\n",
    "  id STRING, \n",
    "  transaction_timestamp STRING, \n",
    "  price STRING,\n",
    "  date DATE GENERATED ALWAYS AS (\n",
    "    cast(cast(transaction_timestamp/1e6 AS TIMESTAMP) AS DATE))\n",
    "    COMMENT \"generated based on `transactions_timestamp` column\")\n",
    "\n",
    "/*the generated column name is 'date'*/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a table comment\n",
    "\n",
    "DE 4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE users_pii\n",
    "COMMENT \"Contains PII\"\n",
    "LOCATION \"${da.paths.working_dir}/tmp/users_pii\"\n",
    "PARTITIONED BY (first_touch_date)\n",
    "AS\n",
    "  SELECT *, \n",
    "    cast(cast(user_first_touch_timestamp/1e6 AS TIMESTAMP) AS DATE) first_touch_date, \n",
    "    current_timestamp() updated,\n",
    "    input_file_name() source_file\n",
    "  FROM parquet.`${da.paths.datasets}/raw/users-historical/`;\n",
    "  \n",
    "SELECT * FROM users_pii;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use CREATE OR REPLACE TABLE and INSERT OVERWRITE\n",
    "\n",
    "DE 4.3 and 4.4\n",
    "\n",
    "We can use overwrites to atomically replace all of the data in a table. There are multiple benefits to overwriting tables instead of deleting and recreating tables:\n",
    "\n",
    "Overwriting a table is much faster because it doesn’t need to list the directory recursively or delete any files.\n",
    "The old version of the table still exists; can easily retrieve the old data using Time Travel.\n",
    "It’s an atomic operation. Concurrent queries can still read the table while you are deleting the table.\n",
    "Due to ACID transaction guarantees, if overwriting the table fails, the table will be in its previous state.\n",
    "Spark SQL provides two easy methods to accomplish complete overwrites.\n",
    "\n",
    "Some students may have noticed previous lesson on CTAS statements actually used CRAS statements (to avoid potential errors if a cell was run multiple times).\n",
    "\n",
    "CREATE OR REPLACE TABLE (CRAS) statements fully replace the contents of a table each time they execute.\n",
    "\n",
    "INSERT OVERWRITE:\n",
    "\n",
    "Can only overwrite an existing table, not create a new one like our CRAS statement\n",
    "Can overwrite only with new records that match the current table schema -- and thus can be a \"safer\" technique for overwriting an existing table without disrupting downstream consumers\n",
    "Can overwrite individual partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE my_table\n",
    "\n",
    "/*CRAS STATEMENT*/\n",
    "CREATE OR REPLACE TABLE events AS\n",
    "SELECT * FROM parquet.`${da.paths.datasets}/raw/events-historical`\n",
    "\n",
    "/*INSERT OVERWRITE*/\n",
    "INSERT OVERWRITE sales\n",
    "SELECT * FROM parquet.`${da.paths.datasets}/raw/sales-historical/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSERT OVERWRITE vs. Append Operation (INSERT INTO) DE4.4\n",
    "\n",
    "Why? INSERT INTO is more effecient than an entire overwrite, and is an atomic way to append data\n",
    "\n",
    "Why Not? No guarantees that data is free from duplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "INSERT INTO sales\n",
    "SELECT * FROM parquet.`${da.paths.datasets}/raw/sales-30m`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify a scenario in which MERGE should be used\n",
    "\n",
    "Upsert & to guarantee Atomicity\n",
    "\n",
    "DE 4.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MERGE UPDATES and its benefits\n",
    "\n",
    "You can upsert data from a source table, view, or DataFrame into a target Delta table using the MERGE SQL operation. Delta Lake supports inserts, updates and deletes in MERGE, and supports extended syntax beyond the SQL standards to facilitate advanced use cases.\n",
    "\n",
    "The main benefits of MERGE:\n",
    "\n",
    "updates, inserts, and deletes are completed as a single transaction\n",
    "multiple conditionals can be added in addition to matching fields\n",
    "provides extensive options for implementing custom logic\n",
    "very useful for preventing duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "/*BASIC SYNTAX*/\n",
    "MERGE INTO target a\n",
    "USING source b\n",
    "ON {merge_condition}\n",
    "WHEN MATCHED THEN {matched_action}\n",
    "WHEN NOT MATCHED THEN {not_matched_action}\n",
    "\n",
    "\n",
    "MERGE INTO users a\n",
    "USING users_update b\n",
    "ON a.user_id = b.user_id\n",
    "WHEN MATCHED AND a.email IS NULL AND b.email IS NOT NULL THEN\n",
    "  UPDATE SET email = b.email, updated = b.updated\n",
    "WHEN NOT MATCHED THEN INSERT *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify MERGE as a command to deduplicate data upon writing\n",
    "\n",
    "Called an insert only merge\n",
    "\n",
    "A common ETL use case is to collect logs or other every-appending datasets into a Delta table through a series of append operations.\n",
    "Many source systems can generate duplicate records. With merge, you can avoid inserting the duplicate records by performing an insert-only merge.\n",
    "This optimized command uses the same MERGE syntax but only provided a WHEN NOT MATCHED clause.\n",
    "Below, we use this to confirm that records with the same user_id and event_timestamp aren't already in the events table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "/*INSERT only MERGE*/\n",
    "MERGE INTO events a\n",
    "USING events_update b\n",
    "ON a.user_id = b.user_id AND a.event_timestamp = b.event_timestamp\n",
    "WHEN NOT MATCHED AND b.traffic_source = 'email' THEN \n",
    "  INSERT *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify why a COPY INTO statement is not duplicating data in the target table\n",
    "\n",
    "DE 4.4\n",
    "\n",
    "\n",
    "COPY INTO provides SQL engineers an idempotent option to incrementally ingest data from external systems. Idempotent = multiple writes or deletes don't cause duplication\n",
    "\n",
    "Note that this operation does have some expectations:\n",
    "\n",
    "Data schema should be consistent\n",
    "Duplicate records should try to be excluded or handled downstream\n",
    "This operation is potentially much cheaper than full table scans for data that grows predictably.\n",
    "\n",
    "**Difference to autoloader\n",
    "\n",
    "COPY INTO = THOUSANDS of FILES, initial setup, record set is not expected to grow/stream\n",
    "AUTO LOADER = MILLIONS OF FILES, record set is expected to grow\n",
    "\n",
    "While here we'll show simple execution on a static directory, the real value is in multiple executions over time picking up new files in the source automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "COPY INTO sales\n",
    "FROM \"${da.paths.datasets}/raw/sales-30m\"\n",
    "FILEFORMAT = PARQUET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use COPY INTO to insert data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "COPY INTO sales\n",
    "FROM \"${da.paths.datasets}/raw/sales-30m\"\n",
    "FILEFORMAT = PARQUET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the components necessary to create a new DLT pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "/*Create Bronze table*/\n",
    "\n",
    "CREATE OR REFRESH STREAMING LIVE TABLE sales_orders_raw\n",
    "COMMENT \"The raw sales orders, ingested from /databricks-datasets.\"\n",
    "AS SELECT * FROM cloud_files(\"/databricks-datasets/retail-org/sales_orders/\", \"json\", map(\"cloudFiles.inferColumnTypes\", \"true\"))\n",
    "\n",
    "/*Create a Silver Table*/\n",
    "CREATE OR REFRESH STREAMING LIVE TABLE sales_orders_cleaned(\n",
    "  CONSTRAINT valid_order_number EXPECT (order_number IS NOT NULL) ON VIOLATION DROP ROW\n",
    ")\n",
    "COMMENT \"The cleaned sales orders with valid order_number(s) and partitioned by order_datetime.\"\n",
    "AS\n",
    "  SELECT f.customer_id, f.customer_name, f.number_of_line_items, \n",
    "         timestamp(from_unixtime((cast(f.order_datetime as long)))) as order_datetime, \n",
    "         date(from_unixtime((cast(f.order_datetime as long)))) as order_date, \n",
    "         f.order_number, f.ordered_products, c.state, c.city, c.lon, c.lat, c.units_purchased, c.loyalty_segment\n",
    "  FROM STREAM(LIVE.sales_orders_raw) f /*The STREAM() function is required around dataset names when specifying other tables or views in your pipeline as a streaming source. Note this is the table created above*/\n",
    "  LEFT JOIN LIVE.customers c\n",
    "    ON c.customer_id = f.customer_id\n",
    "    AND c.customer_name = f.customer_name\n",
    "\n",
    "/*Create a Gold table*/\n",
    "CREATE OR REFRESH LIVE TABLE sales_order_in_la\n",
    "COMMENT \"Sales orders in LA.\"\n",
    "AS\n",
    "  SELECT city, order_date, customer_id, customer_name, ordered_products_explode.curr, \n",
    "         sum(ordered_products_explode.price) as sales, \n",
    "         sum(ordered_products_explode.qty) as quantity, \n",
    "         count(ordered_products_explode.id) as product_count\n",
    "  FROM (SELECT city, order_date, customer_id, customer_name, explode(ordered_products) as ordered_products_explode\n",
    "        FROM LIVE.sales_orders_cleaned \n",
    "        WHERE city = 'Los Angeles')\n",
    "  GROUP BY order_date, city, customer_id, customer_name, ordered_products_explode.curr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CDC with DLT\n",
    "\n",
    "Remember that CDC is used to handle updates to a table such as additions and deletes\n",
    "\n",
    "Default assumption is that rows will contain inserts and updates. So in the example below any book ids matching the books_id key will be updated. Any without will be added.\n",
    "\n",
    "Support applying changes as SCD Type 1 or SCD Type 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "/*General Syntax*/\n",
    "\n",
    "APPLY CHANGES INTO LIVE.target_table \n",
    "FROM STREAM(LIVE.cdc_feed_table)\n",
    "KEYS(key_field) /*primary key fields for table can be one or many*/\n",
    "APPLY AS DELETE WHEN operation_field = \"DELETE\" /*checks for deleted records, this is an optional clause*/\n",
    "SEQUENCE BY sequence_field\n",
    "COLUMNS *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "/*Create bronze table*/\n",
    "CREATE OR REFRESH STREAMING LIVE TABLE books_bronze\n",
    "COMMENT \"the bronze table\"\n",
    "AS SELECT * FROM cloud_files('path',\"json\")\n",
    "\n",
    "/*Create Silver Table*/\n",
    "CREATE OR REFERESH STERAMING LIVE TABLE books silver;\n",
    "\n",
    "/*CDC*/\n",
    "APPLY CHANGES INTO LIVE.books_silver\n",
    "FROM STREAM(LIVE.book_bronze)\n",
    "KEYS(books_id)\n",
    "APPLY AS DELETE WHEN row_status = \"DELETE\"\n",
    "SEQUENCE BY row_time\n",
    "COLUMNS * EXCEPT(row_status, row_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify which source location is utilizing Auto Loader\n",
    "\n",
    "DE 6.0 Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consolidated Incremental Data Ingestion Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoload_to_table(data_source, source_format, table_name, checkpoint_directory):\n",
    "    query = (spark.readStream\n",
    "                  .format(\"cloudFiles\")\n",
    "                  .option(\"cloudFiles.format\", source_format)\n",
    "                  .option(\"cloudFiles.schemaLocation\", checkpoint_directory)\n",
    "                  .load(data_source)\n",
    "                  .writeStream\n",
    "                  .option(\"checkpointLocation\", checkpoint_directory)\n",
    "                  .option(\"mergeSchema\", \"true\")\n",
    "                  .table(table_name))\n",
    "    return query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain change data capture and the behavior of APPLY CHANGES INTO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
