{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART III Prep\n",
    "\n",
    "https://docs.databricks.com/en/sql/language-manual/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to use Hive Variables to substitute in string values derived from the account email of the current user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "SELECT \"${da.db_name}\" AS db_name,\n",
    "       \"${da.paths.working_dir}\" AS working_dir\n",
    "\n",
    "\n",
    "CREATE DATABASE IF NOT EXISTS ${da.db_name}_default_location;\n",
    "CREATE DATABASE IF NOT EXISTS ${da.db_name}_custom_location LOCATION '${da.paths.working_dir}/_custom_location.db';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify where Delta Lake provides ACID transactions\n",
    "\n",
    "Merge update does this AND Delta Lake guarantees that any read against a table will return the most recent version of the table. The purpose of merge is to consolidate update, insert, and other manipulations into one command much like an upsert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "MERGE INTO students b\n",
    "USING updates u\n",
    "ON b.id=u.id\n",
    "WHEN MATCHED AND u.type = \"update\"\n",
    "  THEN UPDATE SET *\n",
    "WHEN MATCHED AND u.type = \"delete\"\n",
    "  THEN DELETE\n",
    "WHEN NOT MATCHED AND u.type = \"insert\"\n",
    "  THEN INSERT *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a managed table (Delta by default as it is backed by delta lake) Metadata + Data Management\n",
    "\n",
    "Managed tables manage underlying data files alongside the metastore registration. Databricks recommends that you use managed tables whenever you create a new table. Unity Catalog managed tables are the default when you create tables in Databricks. They always use Delta Lake. See Work with managed tables.\n",
    "\n",
    "By default, managed tables in a database without the location specified will be created in the dbfs:/user/hive/warehouse/<database_name>.db/ directory.\n",
    "\n",
    "We can see that, as expected, the data and metadata for our Delta Table are stored in that location.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "/*full specification and Unity Catalog Approach*/\n",
    "CREATE TABLE catalog_name.schema_name.table_name\n",
    "  (width INT, length INT, height INT);\n",
    "\n",
    "INSERT INTO managed_default\n",
    "VALUES (3 INT, 2 INT, 1 INT)\n",
    "\n",
    "/*HIVE approach, this will create the table in the default location under dbfs:/user/hive/warehouse/ remember the variables up top*/\n",
    "CREATE DATABASE IF NOT EXISTS ${da.db_name}_default_location;\n",
    "CREATE DATABASE IF NOT EXISTS ${da.db_name}_custom_location LOCATION '${da.paths.working_dir}/_custom_location.db';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an External Table (Metadata managed by Databricks only)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "/*Exammple 1 with HIVE*/\n",
    "CREATE TABLE my_external_table\n",
    "  (width INT, length INT, height INT)\n",
    "LOCATION 'dbfs:/mnt/demo/external_default';\n",
    "\n",
    "  \n",
    "INSERT INTO external_default\n",
    "VALUES (3 INT, 2 INT, 1 INT)\n",
    "\n",
    "hive_metastore.default.managed_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "/*Example 2 with Unity Catalog*/\n",
    "\n",
    "CREATE TABLE my_external_table\n",
    "    (width INT. length INT, height INT)\n",
    "LOCATION 'abfss://<bucket-path>/<table-directory>';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore table Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "/*basic metadata*/\n",
    "DESCRIBE TABLE customer;\n",
    "\n",
    "DESCRIBE TABLE extended my_table_name;\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "source": [
    "Return information about a schema: \n",
    "\n",
    "Returns information about schema, partitioning, table size, and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "/*Managed Table*/\n",
    "DESCRIBE DETAIL [schema_name].table_name\n",
    "\n",
    "DESCRIBE EXTENDED students_table\n",
    "\n",
    "/*External Location*/\n",
    "DESCRIBE EXTERNAL LOCATION table_location_name;\n",
    "\n",
    "/*Show external locations*/\n",
    "SHOW EXTERNAL LOCATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the location of a table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the SHOW table command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "USE <your_database_name>\n",
    "\n",
    "/*show tables*/\n",
    "SHOW TABLES\n",
    "\n",
    "/*show tables in global temp*/\n",
    "SHOW TABLES IN global_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the directory structure of Delta Lake files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(f\"{DA.paths.user_db}/students\"))\n",
    "\n",
    "#using DB Utils\n",
    "fs ls 'dbfs:/user/hive/warehouse/employees'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify who has written previous versions of a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "DESCRIBE HISTORY table_name;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review a history of table transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "DESCRIBE HISTORY my_table_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roll back a table to a previous version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "/*Option 1*/\n",
    "RESTORE TABLE employees TO VERSION AS OF 5\n",
    "\n",
    "/*Option 2*/\n",
    "RESTORE TABLE table_name TO TIMESTAMP AS OF 'yyyy-MM-dd HH:mm:ss';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify that a table can be rolled back to a previous version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "DESCRIBE HISTORY table_name;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query a specific version of a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "SELECT * \n",
    "FROM employees VERSION AS OF 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify CTAS as a solution and create a table using CTAS\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "CTAS statements automatically infer schema information from query results and do not support manual schema declaration.\n",
    "\n",
    "This means that CTAS statements are useful for external data ingestion from sources with well-defined schema, such as Parquet files and tables.\n",
    "\n",
    "CTAS statements also do not support specifying additional file options.\n",
    "\n",
    "We can see how this would present significant limitations when trying to ingest data from CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "/*FROM A TABLE*/\n",
    "CREATE OR REPLACE TABLE my_table AS\n",
    "SELECT * FROM my_other_table\n",
    "\n",
    "\n",
    "/*FROM A FILE*/\n",
    "CREATE OR REPLACE TABLE sales_unparsed AS\n",
    "SELECT * FROM csv.`${da.paths.datasets}/raw/sales-csv/`;\n",
    "SELECT * FROM sales_unparsed;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a generated column\n",
    "\n",
    "DE 4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE purchase_dates (\n",
    "  id STRING, \n",
    "  transaction_timestamp STRING, \n",
    "  price STRING,\n",
    "  date DATE GENERATED ALWAYS AS (\n",
    "    cast(cast(transaction_timestamp/1e6 AS TIMESTAMP) AS DATE))\n",
    "    COMMENT \"generated based on `transactions_timestamp` column\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a table comment\n",
    "\n",
    "DE 4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE users_pii\n",
    "COMMENT \"Contains PII\"\n",
    "LOCATION \"${da.paths.working_dir}/tmp/users_pii\"\n",
    "PARTITIONED BY (first_touch_date)\n",
    "AS\n",
    "  SELECT *, \n",
    "    cast(cast(user_first_touch_timestamp/1e6 AS TIMESTAMP) AS DATE) first_touch_date, \n",
    "    current_timestamp() updated,\n",
    "    input_file_name() source_file\n",
    "  FROM parquet.`${da.paths.datasets}/raw/users-historical/`;\n",
    "  \n",
    "SELECT * FROM users_pii;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use CREATE OR REPLACE TABLE and INSERT OVERWRITE\n",
    "\n",
    "DE 4.3 and 4.4\n",
    "\n",
    "We can use overwrites to atomically replace all of the data in a table. There are multiple benefits to overwriting tables instead of deleting and recreating tables:\n",
    "\n",
    "Overwriting a table is much faster because it doesn’t need to list the directory recursively or delete any files.\n",
    "The old version of the table still exists; can easily retrieve the old data using Time Travel.\n",
    "It’s an atomic operation. Concurrent queries can still read the table while you are deleting the table.\n",
    "Due to ACID transaction guarantees, if overwriting the table fails, the table will be in its previous state.\n",
    "Spark SQL provides two easy methods to accomplish complete overwrites.\n",
    "\n",
    "Some students may have noticed previous lesson on CTAS statements actually used CRAS statements (to avoid potential errors if a cell was run multiple times).\n",
    "\n",
    "CREATE OR REPLACE TABLE (CRAS) statements fully replace the contents of a table each time they execute.\n",
    "\n",
    "INSERT OVERWRITE:\n",
    "\n",
    "Can only overwrite an existing table, not create a new one like our CRAS statement\n",
    "Can overwrite only with new records that match the current table schema -- and thus can be a \"safer\" technique for overwriting an existing table without disrupting downstream consumers\n",
    "Can overwrite individual partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE my_table\n",
    "\n",
    "/*INSERT OVERWRITE*/\n",
    "INSERT OVERWRITE sales\n",
    "SELECT * FROM parquet.`${da.paths.datasets}/raw/sales-historical/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify a scenario in which MERGE should be used\n",
    "\n",
    "Upsert & to guarantee Atomicity\n",
    "\n",
    "DE 4.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "MERGE INTO target a\n",
    "USING source b\n",
    "ON {merge_condition}\n",
    "WHEN MATCHED THEN {matched_action}\n",
    "WHEN NOT MATCHED THEN {not_matched_action}\n",
    "\n",
    "\n",
    "MERGE INTO users a\n",
    "USING users_update b\n",
    "ON a.user_id = b.user_id\n",
    "WHEN MATCHED AND a.email IS NULL AND b.email IS NOT NULL THEN\n",
    "  UPDATE SET email = b.email, updated = b.updated\n",
    "WHEN NOT MATCHED THEN INSERT *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify MERGE as a command to deduplicate data upon writing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify why a COPY INTO statement is not duplicating data in the target table\n",
    "\n",
    "DE 4.4\n",
    "\n",
    "\n",
    "COPY INTO provides SQL engineers an idempotent option to incrementally ingest data from external systems.\n",
    "\n",
    "Note that this operation does have some expectations:\n",
    "\n",
    "Data schema should be consistent\n",
    "Duplicate records should try to be excluded or handled downstream\n",
    "This operation is potentially much cheaper than full table scans for data that grows predictably.\n",
    "\n",
    "While here we'll show simple execution on a static directory, the real value is in multiple executions over time picking up new files in the source automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "COPY INTO sales\n",
    "FROM \"${da.paths.datasets}/raw/sales-30m\"\n",
    "FILEFORMAT = PARQUET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use COPY INTO to insert data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "COPY INTO sales\n",
    "FROM \"${da.paths.datasets}/raw/sales-30m\"\n",
    "FILEFORMAT = PARQUET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the components necessary to create a new DLT pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "/*Create Bronze table*/\n",
    "\n",
    "CREATE OR REFRESH STREAMING LIVE TABLE sales_orders_raw\n",
    "COMMENT \"The raw sales orders, ingested from /databricks-datasets.\"\n",
    "AS SELECT * FROM cloud_files(\"/databricks-datasets/retail-org/sales_orders/\", \"json\", map(\"cloudFiles.inferColumnTypes\", \"true\"))\n",
    "\n",
    "/*Create a Silver Table*/\n",
    "CREATE OR REFRESH STREAMING LIVE TABLE sales_orders_cleaned(\n",
    "  CONSTRAINT valid_order_number EXPECT (order_number IS NOT NULL) ON VIOLATION DROP ROW\n",
    ")\n",
    "COMMENT \"The cleaned sales orders with valid order_number(s) and partitioned by order_datetime.\"\n",
    "AS\n",
    "  SELECT f.customer_id, f.customer_name, f.number_of_line_items, \n",
    "         timestamp(from_unixtime((cast(f.order_datetime as long)))) as order_datetime, \n",
    "         date(from_unixtime((cast(f.order_datetime as long)))) as order_date, \n",
    "         f.order_number, f.ordered_products, c.state, c.city, c.lon, c.lat, c.units_purchased, c.loyalty_segment\n",
    "  FROM STREAM(LIVE.sales_orders_raw) f\n",
    "  LEFT JOIN LIVE.customers c\n",
    "    ON c.customer_id = f.customer_id\n",
    "    AND c.customer_name = f.customer_name\n",
    "\n",
    "/*Create a Gold table*/\n",
    "CREATE OR REFRESH LIVE TABLE sales_order_in_la\n",
    "COMMENT \"Sales orders in LA.\"\n",
    "AS\n",
    "  SELECT city, order_date, customer_id, customer_name, ordered_products_explode.curr, \n",
    "         sum(ordered_products_explode.price) as sales, \n",
    "         sum(ordered_products_explode.qty) as quantity, \n",
    "         count(ordered_products_explode.id) as product_count\n",
    "  FROM (SELECT city, order_date, customer_id, customer_name, explode(ordered_products) as ordered_products_explode\n",
    "        FROM LIVE.sales_orders_cleaned \n",
    "        WHERE city = 'Los Angeles')\n",
    "  GROUP BY order_date, city, customer_id, customer_name, ordered_products_explode.curr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify which source location is utilizing Auto Loader\n",
    "\n",
    "DE 6.0 Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain change data capture and the behavior of APPLY CHANGES INTO"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
