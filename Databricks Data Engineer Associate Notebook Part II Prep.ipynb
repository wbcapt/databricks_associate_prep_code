{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select data from a file or directory of files. Databricks supports these locations:\n",
    "\n",
    "Unity Catalog volumes\n",
    "\n",
    "Workspace files\n",
    "\n",
    "Cloud object storage\n",
    "\n",
    "DBFS mounts and DBFS root\n",
    "\n",
    "Ephemeral storage attached to the driver node of the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL and DatabricksSQL\n",
    "https://docs.databricks.com/en/files/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spark SQL & Databricks SQL\n",
    "\n",
    "#single file, unity catalog volume (for non tabular data files stored in cloud object storage)\n",
    "SELECT * FROM csv.`/Volumes/my_catalog/my_schema/my_volume/data.csv`;\n",
    "LIST '/Volumes/my_catalog/my_schema/my_volume/';\n",
    "\n",
    "\n",
    "#single file, workspace files (files in a workspace that aren't notebooks. 500MB limit)\n",
    "SELECT * FROM json.'file:/Workspace/Users/<user-folder>/file.json'\n",
    "\n",
    "#single file, cloud object store through unity catalog\n",
    "SELECT * FROM csv.`abfss://container-name@storage-account-name.dfs.core.windows.net/path/file.json`; \n",
    "LIST 'abfss://container-name@storage-account-name.dfs.core.windows.net/path';\n",
    "\n",
    "#single file, from DBFS mount (not securable by Unity Catalog)\n",
    "SELECT * FROM json.`/mnt/path/to/data.json`;\n",
    "\n",
    "#from a directory\n",
    "SELECT * FROM json.`path/to/file.json`\n",
    "\n",
    "#wildcard match\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#single file, unity catalog volume (for non tabular data files stored in cloud object storage)\n",
    "spark.read.format(\"json\").load(\"/volumes/my_catalog/my_schema/my_volumne/data.json\").show()\n",
    "\n",
    "#single file, workspace files (files in a workspace that aren't notebooks. 500MB limit)\n",
    "spark.read.format(\"json\").load(\"file:/Workspace/Users/<user-folder>/data.json\").show()\n",
    "\n",
    "#single file, cloud object store through unity catalog\n",
    "spark.read.format(\"json\").load(\"abfss://container-name@storage-account-name.dfs.core.windows.net/path/file.json\").show()\n",
    "\n",
    "#single file, from DBFS mount (not securable by Unity Catalog)\n",
    "spark.read.format(\"json\").load(\"/mnt/path/to/data.json\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python/Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#single file, unity catalog volume (for non tabular data files stored in cloud object storage)\n",
    "#Pandas\n",
    "df = pd.read_csv('/Volumes/my_catalog/my_schema/my_volume/data.csv')\n",
    "#Python\n",
    "df = spark.read.format(\"csv\").load(\"/Volumes/catalog_name/schema_name/volume_name/data.csv\")\n",
    "\n",
    "#single file, workspace files (files in a workspace that aren't notebooks. 500MB limit)\n",
    "df = pd.read_csv('/Workspace/Users/<user-folder>/data.csv')\n",
    "\n",
    "#cloud object storage.\n",
    "#Not Supported\n",
    "\n",
    "#DBFS\n",
    "df = pd.read_csv('/dbfs/mnt/path/to/data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a view, a temporary view, and a CTE as a reference to a file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a table from a JDBC connection and from an external CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deduplicate rows from an existing Delta Lake table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new table from an existing table while removing duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deduplicate a row based on specific columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse JSON strings into structs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the PIVOT clause as a way to convert data from a long format to a wide format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a SQL UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the location of a function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "readStream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read = (spark.readStream\n",
    "    .format('cloudFiles')\n",
    "    .option('cloudFiles.format', 'csv')\n",
    "    .option('cloudFiles.schemaLocation', {path})\n",
    "    .option('header', 'true')\n",
    "    .load('path')\n",
    "    .schema(schema)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read.writeStream(\n",
    "    .format('delta')\n",
    "    .option('cloudFiles.checkpointLocation')\n",
    "    .outputMode('append')\n",
    "    .query('myQuery_name')\n",
    "    .trigger(availableNow=True)\n",
    "    .toTable('my_table_name')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
