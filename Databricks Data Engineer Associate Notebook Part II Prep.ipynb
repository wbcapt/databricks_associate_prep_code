{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select data from a file or directory of files. Databricks supports these locations:\n",
    "\n",
    "Unity Catalog volumes\n",
    "\n",
    "Workspace files\n",
    "\n",
    "Cloud object storage\n",
    "\n",
    "DBFS mounts and DBFS root\n",
    "\n",
    "Ephemeral storage attached to the driver node of the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL and DatabricksSQL\n",
    "https://docs.databricks.com/en/files/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spark SQL & Databricks SQL\n",
    "\n",
    "#single file, unity catalog volume (for non tabular data files stored in cloud object storage)\n",
    "SELECT * FROM csv.`/Volumes/my_catalog/my_schema/my_volume/data.csv`;\n",
    "LIST '/Volumes/my_catalog/my_schema/my_volume/';\n",
    "\n",
    "\n",
    "#single file, workspace files (files in a workspace that aren't notebooks. 500MB limit)\n",
    "SELECT * FROM json.'file:/Workspace/Users/<user-folder>/file.json'\n",
    "\n",
    "#single file, cloud object store through unity catalog\n",
    "SELECT * FROM csv.`abfss://container-name@storage-account-name.dfs.core.windows.net/path/file.json`; \n",
    "LIST 'abfss://container-name@storage-account-name.dfs.core.windows.net/path';\n",
    "\n",
    "#single file, from DBFS mount (not securable by Unity Catalog)\n",
    "SELECT * FROM json.`/mnt/path/to/data.json`;\n",
    "\n",
    "#from a directory\n",
    "SELECT * FROM json.`path/to/file.json`;\n",
    "\n",
    "#wildcard match\n",
    "SELECT * FROM csv.`dir/path/*.csv`;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#single file, unity catalog volume (for non tabular data files stored in cloud object storage)\n",
    "spark.read.format(\"json\").load(\"/volumes/my_catalog/my_schema/my_volumne/data.json\").show()\n",
    "\n",
    "#single file, workspace files (files in a workspace that aren't notebooks. 500MB limit)\n",
    "spark.read.format(\"json\").load(\"file:/Workspace/Users/<user-folder>/data.json\").show()\n",
    "\n",
    "#single file, cloud object store through unity catalog\n",
    "spark.read.format(\"json\").load(\"abfss://container-name@storage-account-name.dfs.core.windows.net/path/file.json\").show()\n",
    "\n",
    "#single file, from DBFS mount (not securable by Unity Catalog)\n",
    "spark.read.format(\"json\").load(\"/mnt/path/to/data.json\").show()\n",
    "\n",
    "#from a directory\n",
    "spark.read.format(\"json\").load('path/*.json')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python/Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#single file, unity catalog volume (for non tabular data files stored in cloud object storage)\n",
    "#Pandas\n",
    "df = pd.read_csv('/Volumes/my_catalog/my_schema/my_volume/data.csv')\n",
    "#Python\n",
    "df = spark.read.format(\"csv\").load(\"/Volumes/catalog_name/schema_name/volume_name/data.csv\")\n",
    "\n",
    "#single file, workspace files (files in a workspace that aren't notebooks. 500MB limit)\n",
    "df = pd.read_csv('/Workspace/Users/<user-folder>/data.csv')\n",
    "\n",
    "#cloud object storage.\n",
    "#Not Supported\n",
    "\n",
    "#DBFS\n",
    "df = pd.read_csv('/dbfs/mnt/path/to/data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a view, a temporary view, and a CTE as a reference to a file\n",
    "\n",
    "IF NOT EXISTS is something that is not supported\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE TEMP VIEW books_tmp_vw\n",
    "      (book_id STRING, title STRING, author STRING, category STRING, price DOUBLE)\n",
    "    USING CSV\n",
    "    OPTIONS (\n",
    "      path = \"${dataset.bookstore}/books-csv/export_*.csv\",\n",
    "      header = \"true\",\n",
    "      delimiter = \";\"\n",
    "    );\n",
    "\n",
    "/*Alternative*/\n",
    "\n",
    "    CREATE OR REPLACE TABLE csv_table\n",
    "    USING CSV\n",
    "    OPTIONS (\n",
    "      path '/path/to/your/csv/file.csv',\n",
    "      header 'true',\n",
    "      delimiter ','\n",
    "    );\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a table from a JDBC connection and from an external CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE DATABASE IF NOT EXISTS jdbc_db;\n",
    "\n",
    "    %sql\n",
    "    CREATE TABLE jdbc_table\n",
    "    USING jdbc\n",
    "    OPTIONS (\n",
    "      url 'jdbc:mysql://your_database_hostname:3306/your_database_name',\n",
    "      user 'your_username',\n",
    "      password 'your_password',\n",
    "      dbtable '(SELECT * FROM your_table_name)'\n",
    "    );\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify a table location using the extented description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    " DESCRIBE EXTENDED managed_table_in_db_with_custom_location;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deduplicate rows from an existing Delta Lake table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").load(table_path)\n",
    "        deduplicated_df = df.dropDuplicates()\n",
    "        deduplicated_df.write.format(\"delta\").mode(\"overwrite\").save(table_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new table from an existing table while removing duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE TABLE my_table AS\n",
    "\n",
    "SELECT\n",
    "*\n",
    "FROM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deduplicate a row based on specific columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    " deduplicated_df = df.dropDuplicates([\"column1\", \"column2\"])\n",
    "\n",
    "    SELECT COUNT(DISTINCT(user_id, user_first_touch_timestamp))\n",
    "    FROM users_dirty\n",
    "    WHERE user_id IS NOT NULL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse JSON strings into structs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TEMP VIEW parsed_customers AS\n",
    "  SELECT \n",
    "      customer_id\n",
    "      ,from_json(profile, \n",
    "                  schema_of_json('{\"first_name\":\"Thomas\",\n",
    "                              \"last_name\":\"Lane\",\"gender\":\"Male\",\n",
    "                              \"address\":{\"street\":\"06 Boulevard Victor Hugo\",\n",
    "                              \"city\":\"Paris\",\"country\":\"France\"}}'\n",
    "                              )\n",
    "                  ) \n",
    "        AS profile_struct\n",
    "  FROM customers;\n",
    "  \n",
    "SELECT * FROM parsed_customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the PIVOT clause as a way to convert data from a long format to a wide format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE transactions AS\n",
    "\n",
    "    SELECT * FROM (\n",
    "      SELECT\n",
    "        customer_id,\n",
    "        book.book_id AS book_id,\n",
    "        book.quantity AS quantity\n",
    "      FROM orders_enriched\n",
    "    ) PIVOT (\n",
    "      sum(quantity) FOR book_id in (\n",
    "        'B01', 'B02', 'B03', 'B04', 'B05', 'B06',\n",
    "        'B07', 'B08', 'B09', 'B10', 'B11', 'B12'\n",
    "      )\n",
    "    );\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a SQL UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE FUNCTION yelling(text STRING)\n",
    "    RETURNS STRING\n",
    "    RETURN concat(upper(text), \"!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the location of a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    " DESCRIBE FUNCTION EXTENDED get_url\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "readStream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read = (spark.readStream\n",
    "    .format('cloudFiles')\n",
    "    .option('cloudFiles.format', 'csv')\n",
    "    .option('cloudFiles.schemaLocation', {path})\n",
    "    .option('header', 'true')\n",
    "    .load('path')\n",
    "    .schema(schema)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read.writeStream(\n",
    "    .format('delta')\n",
    "    .option('cloudFiles.checkpointLocation')\n",
    "    .outputMode('append')\n",
    "    .query('myQuery_name')\n",
    "    .trigger(availableNow=True)\n",
    "    .toTable('my_table_name')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
