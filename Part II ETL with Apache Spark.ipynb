{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select data from a file or directory of files. Databricks supports these locations:\n",
    "\n",
    "Unity Catalog volumes\n",
    "\n",
    "Workspace files\n",
    "\n",
    "Cloud object storage\n",
    "\n",
    "DBFS mounts and DBFS root\n",
    "\n",
    "Ephemeral storage attached to the driver node of the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL and DatabricksSQL\n",
    "https://docs.databricks.com/en/files/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spark SQL & Databricks SQL\n",
    "\n",
    "#single file, unity catalog volume (for non tabular data files stored in cloud object storage)\n",
    "SELECT * FROM csv.`/Volumes/my_catalog/my_schema/my_volume/data.csv`;\n",
    "LIST '/Volumes/my_catalog/my_schema/my_volume/';\n",
    "\n",
    "\n",
    "#single file, workspace files (files in a workspace that aren't notebooks. 500MB limit)\n",
    "SELECT * FROM json.'file:/Workspace/Users/<user-folder>/file.json'\n",
    "\n",
    "#single file, cloud object store through unity catalog\n",
    "SELECT * FROM csv.`abfss://container-name@storage-account-name.dfs.core.windows.net/path/file.json`; \n",
    "LIST 'abfss://container-name@storage-account-name.dfs.core.windows.net/path';\n",
    "\n",
    "#single file, from DBFS mount (not securable by Unity Catalog)\n",
    "SELECT * FROM json.`/mnt/path/to/data.json`;\n",
    "\n",
    "#from a directory\n",
    "SELECT * FROM json.`path/to/directory`;\n",
    "\n",
    "#wildcard match\n",
    "SELECT * FROM csv.`dir/path/*.csv`;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#single file, unity catalog volume (for non tabular data files stored in cloud object storage)\n",
    "spark.read.format(\"json\").load(\"/volumes/my_catalog/my_schema/my_volumne/data.json\").show()\n",
    "\n",
    "#single file, workspace files (files in a workspace that aren't notebooks. 500MB limit)\n",
    "spark.read.format(\"json\").load(\"file:/Workspace/Users/<user-folder>/data.json\").show()\n",
    "\n",
    "#single file, cloud object store through unity catalog\n",
    "spark.read.format(\"json\").load(\"abfss://container-name@storage-account-name.dfs.core.windows.net/path/file.json\").show()\n",
    "\n",
    "#single file, from DBFS mount (not securable by Unity Catalog)\n",
    "spark.read.format(\"json\").load(\"/mnt/path/to/data.json\").show()\n",
    "\n",
    "#from a directory\n",
    "spark.read.format(\"json\").load('path/*.json')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python/Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#single file, unity catalog volume (for non tabular data files stored in cloud object storage)\n",
    "#Pandas\n",
    "df = pd.read_csv('/Volumes/my_catalog/my_schema/my_volume/data.csv')\n",
    "#Python\n",
    "df = spark.read.format(\"csv\").load(\"/Volumes/catalog_name/schema_name/volume_name/data.csv\")\n",
    "\n",
    "#single file, workspace files (files in a workspace that aren't notebooks. 500MB limit)\n",
    "df = pd.read_csv('/Workspace/Users/<user-folder>/data.csv')\n",
    "\n",
    "#cloud object storage.\n",
    "#Not Supported\n",
    "\n",
    "#DBFS\n",
    "df = pd.read_csv('/dbfs/mnt/path/to/data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a view, a temporary view, and a CTE as a reference to a file\n",
    "\n",
    "See DE 4.1 Notebook\n",
    "\n",
    "standard view = presists\n",
    "temp view = notebook session scope\n",
    "global temp view = cluster session scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE GLOBAL TEMP VIEW my_temp_vw\n",
    "AS\n",
    "SELECT * FROM json.'my/file/path.json'\n",
    "\n",
    "/*Now select*/\n",
    "SELECT * FROM my_temp_vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE TEMP VIEW books_tmp_vw\n",
    "      (book_id STRING, title STRING, author STRING, category STRING, price DOUBLE)\n",
    "    USING CSV\n",
    "    OPTIONS (\n",
    "      path = \"${dataset.bookstore}/books-csv/export_*.csv\",\n",
    "      header = \"true\",\n",
    "      delimiter = \";\"\n",
    "    );\n",
    "\n",
    "/*Alternative*/\n",
    "\n",
    "    CREATE OR REPLACE TABLE csv_table\n",
    "    USING CSV\n",
    "    OPTIONS (\n",
    "      path '/path/to/your/csv/file.csv',\n",
    "      header = 'true',\n",
    "      delimiter = ','\n",
    "    );\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "USE ${da.db_name}_default_location;\n",
    "/*${hive variable}*/\n",
    "\n",
    "CREATE OR REPLACE TEMPORARY VIEW temp_delays USING CSV OPTIONS (\n",
    "  path = '${da.paths.working_dir}/flights/departuredelays.csv', \n",
    "  header = \"true\",\n",
    "  mode = \"FAILFAST\" -- abort file parsing with a RuntimeException if any malformed lines are encountered\n",
    ");\n",
    "CREATE OR REPLACE TABLE external_table LOCATION '${da.paths.working_dir}/external_table' AS\n",
    "  SELECT * FROM temp_delays;\n",
    "\n",
    "SELECT * FROM external_table;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify that tables from external sources are not Delta Lake tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "DESCRIBE EXTENDED my_external_table_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a table from a JDBC connection or an external database\n",
    "\n",
    "See DE 4.2 notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE DATABASE IF NOT EXISTS jdbc_db;\n",
    "\n",
    "    CREATE TABLE jdbc_table\n",
    "    USING jdbc\n",
    "    OPTIONS (\n",
    "          url = \"jdbc:{databaseServerType}://{jdbcHostname}:{jdbcPort}\",\n",
    "          dbtable = \"{jdbcDatabase}.table\",\n",
    "          user = \"{jdbcUsername}\",\n",
    "          password = \"{jdbcPassword}\"\n",
    "    );\n",
    "\n",
    "/*Full Example with sql server*/\n",
    "\n",
    "CREATE TABLE my_table_name\n",
    "USING jdbc\n",
    "OPTIONS(\n",
    "  url 'jdbc:sqlserver://acctsql.public.84b583495e70.database.windows.net:3342',\n",
    "  dtable 'mpp.AR_2'\n",
    "  user 'sqlsvcACBJ'\n",
    "  password 'asdafasfsahha'\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "/*Full example*/\n",
    "DROP TABLE IF EXISTS users_jdbc;\n",
    "\n",
    "CREATE TABLE users_jdbc\n",
    "USING JDBC\n",
    "OPTIONS (\n",
    "  url = \"jdbc:sqlite:/${da.username}_ecommerce.db\",\n",
    "  dbtable = \"users\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a table from an external CSV file\n",
    "\n",
    "DE 4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TEMP VIEW sales_tmp_vw\n",
    "  (order_id LONG, email STRING, transactions_timestamp LONG, total_item_quantity INTEGER, purchase_revenue_in_usd DOUBLE, unique_items INTEGER, items STRING)\n",
    "USING CSV\n",
    "OPTIONS (\n",
    "  path \"${da.paths.datasets}/raw/sales-csv\",\n",
    "  header \"true\",\n",
    "  delimiter \"|\"\n",
    ");\n",
    "\n",
    "CREATE TABLE sales_delta AS\n",
    "  SELECT * FROM sales_tmp_vw;\n",
    "  \n",
    "SELECT * FROM sales_delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify a table location using the extented description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    " DESCRIBE EXTENDED managed_table_in_db_with_custom_location;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a basic Delta Table and insert values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE TABLE my_table_name\n",
    "(id INT, name STRING, value DOUBLE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "/*Option 1*/\n",
    "INSERT INTO my_table_name VALUES (1,\"steve\",1.0);\n",
    "INSERT INTO my_table_name VALUES (2,\"leve\", 2.0);\n",
    "INSERT INTO my_table_name VALUES (3,\"keve\", 2.0);\n",
    "\n",
    "/*Option 2*/\n",
    "INSERT INTO my_table_name\n",
    "VALUES\n",
    "    (4,\"bro\", 22.0),\n",
    "    (5,\"bre\", 33.0),\n",
    "    (6,\"mot\", 324.0),\n",
    "    (7,\"lem\", 24.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deduplicate rows from an existing Delta Lake table\n",
    "DE 4.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").load(table_path)\n",
    "        deduplicated_df = df.dropDuplicates()\n",
    "        deduplicated_df.write.format(\"delta\").mode(\"overwrite\").save(table_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TEMP VIEW users_deduped AS\n",
    "  SELECT DISTINCT(*) FROM users_dirty;\n",
    "\n",
    "SELECT * FROM users_deduped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new table from an existing table while removing duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE TABLE my_table AS\n",
    "\n",
    "SELECT\n",
    "*\n",
    "FROM \n",
    "some_other_place"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deduplicate a row based on specific columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    " deduplicated_df = df.dropDuplicates([\"column1\", \"column2\"])\n",
    "\n",
    "    SELECT COUNT(DISTINCT(user_id, user_first_touch_timestamp))\n",
    "    FROM users_dirty\n",
    "    WHERE user_id IS NOT NULL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract a calendar date from a timestamp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CAST(my_timestamp AS DATE) AS my_date_field\n",
    "\n",
    "/*Altenative to get rid of microseconds in UNIX (the 1e6 part)*/\n",
    "CAST(my_timestamp/1e6 AS timestamp) my_date_field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract a specific pattern from an existing string column\n",
    "\n",
    "DE4.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "SELECT *,\n",
    "  date_format(first_touch, \"MMM d, yyyy\") AS first_touch_date,\n",
    "  date_format(first_touch, \"HH:mm:ss\") AS first_touch_time,\n",
    "  regexp_extract(email, \"(?<=@).+\", 0) AS email_domain\n",
    "FROM (\n",
    "  SELECT *,\n",
    "    CAST(user_first_touch_timestamp / 1e6 AS timestamp) AS first_touch \n",
    "  FROM deduped_users\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilize the dot \":\" syntax to extract nested data fields\n",
    "\n",
    "Remember that JSON is key value pairs\n",
    "\n",
    "DE4.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "/*In most cases, Kafka data will be binary-encoded JSON values. We'll cast the key and value as strings below to look at these in a human-readable format.*/\n",
    "CREATE OR REPLACE TEMP VIEW events_strings AS\n",
    "  SELECT string(key), string(value) \n",
    "  FROM events_raw;\n",
    "  \n",
    "SELECT * FROM events_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "/*Spark SQL has built-in functionality to directly interact with JSON data stored as strings. We can use the : syntax to traverse nested data structures. Remember to create the view up top first*/\n",
    "\n",
    "SELECT value:device, value:geo:city \n",
    "FROM events_strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse JSON strings into structs.\n",
    "\n",
    "A JSON struct is a nested data structure that can include other JSON objects or arrays. It’s similar to a dictionary or map in programming languages, where each key is associated with a value. Values can be strings, numbers, arrays, or other JSON objects.\n",
    "\n",
    "Databricks provides various functions and methods for working with JSON data, especially when you need to handle nested or semi-structured data. You can read JSON data into a DataFrame and then use Spark SQL functions to query and manipulate it.\n",
    "\n",
    "DE4.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "jsonc"
    }
   },
   "outputs": [],
   "source": [
    "##Example of JSON\n",
    "\n",
    "{\n",
    "    \"name\": \"John Doe\",\n",
    "    \"age\": 30,\n",
    "    \"address\": {\n",
    "      \"street\": \"123 Elm St\",\n",
    "      \"city\": \"Somewhere\"\n",
    "    },\n",
    "    \"phones\": [\"123-456-7890\", \"987-654-3210\"]\n",
    "  }\n",
    "\n",
    "\n",
    "##Explanation\n",
    "In this JSON object:\n",
    "\n",
    "\"name\" is a string.\n",
    "\"age\" is a number.\n",
    "\"address\" is another JSON object (struct) with \"street\" and \"city\".\n",
    "\"phones\" is an array of strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Parsing \n",
    "\n",
    "Spark SQL also has a schema_of_json function to derive the JSON schema from an example. Here, we copy and paste an example JSON to the function and chain it into the from_json function to cast our value field to a struct type.\n",
    "\n",
    "DE 4.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TEMP VIEW parsed_customers AS\n",
    "  SELECT \n",
    "      customer_id\n",
    "      ,from_json(profile, \n",
    "                  schema_of_json('{\"first_name\":\"Thomas\",\n",
    "                              \"last_name\":\"Lane\",\"gender\":\"Male\",\n",
    "                              \"address\":{\"street\":\"06 Boulevard Victor Hugo\",\n",
    "                              \"city\":\"Paris\",\"country\":\"France\"}}'\n",
    "                              )\n",
    "                  ) \n",
    "        AS profile_struct\n",
    "  FROM customers;\n",
    "  \n",
    "SELECT * FROM parsed_customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Once a JSON string is unpacked to a struct type, Spark supports * (star) unpacking to flatten fields into columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TEMP VIEW new_events_final AS\n",
    "  SELECT json.* \n",
    "  FROM parsed_events;\n",
    "  \n",
    "SELECT * FROM new_events_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilize the dot \".\" syntax to extract nested data fields\n",
    "\n",
    "This can be used to Explore Data Structures\n",
    "Spark SQL has robust syntax for working with complex and nested data types.\n",
    "\n",
    "The ecommerce field is a struct that contains a double and 2 longs.\n",
    "\n",
    "We can interact with the subfields in this field using standard . syntax similar to how we might traverse nested data in JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "SELECT ecommerce.purchase_revenue_in_usd \n",
    "FROM events\n",
    "WHERE ecommerce.purchase_revenue_in_usd IS NOT NULL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify which result will be returned based on a join query.\n",
    "\n",
    "This is just standard joins: INNER, OUTER, LEFT, RIGHT, ANTI, CROSS, SEMI\n",
    "\n",
    "CROSS = cartesian product\n",
    "SEMI = everything in left matching right, but only show left table columns\n",
    "ANTI = everything in left not matching right, but only show left table columns\n",
    "\n",
    "SET OPERATORS: UNION, MINUS, INTERSET\n",
    "\n",
    "INTERSECT = only matching records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the PIVOT clause as a way to convert data from a long format to a wide format\n",
    "DE4.7\n",
    "\n",
    "Purpose = flatten information and aggregate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE transactions AS\n",
    "\n",
    "    SELECT * FROM (\n",
    "      SELECT\n",
    "        customer_id,\n",
    "        book.book_id AS book_id,\n",
    "        book.quantity AS quantity\n",
    "      FROM orders_enriched\n",
    "    ) PIVOT (\n",
    "      sum(quantity) FOR book_id in (\n",
    "        'B01', 'B02', 'B03', 'B04', 'B05', 'B06',\n",
    "        'B07', 'B08', 'B09', 'B10', 'B11', 'B12'\n",
    "      )\n",
    "    );\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify a scenario to use the explode function versus the flatten function\n",
    "\n",
    "Explode = puts each element in an array on its own row\n",
    "\n",
    "Flatten = allows multiple arrays to be combined into a single array\n",
    "\n",
    "Array_distinct = removes duplicate elements from an rray\n",
    "\n",
    "Collect_Set = collect unique values for a field, including fields within arrays\n",
    "\n",
    "DE4.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "/*Explode: puts each element in the items array field (a struct) into its own row*/\n",
    "\n",
    "SELECT user_id, event_timestamp, event_name, explode(items) AS item \n",
    "FROM events\n",
    "\n",
    "/*collect and flatten*/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a SQL UDF\n",
    "\n",
    "Basic syntax  = function name, optional parameters, type returned, and custom logic\n",
    "\n",
    "DE4.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE FUNCTION yelling(text STRING)\n",
    "    RETURNS STRING\n",
    "    RETURN concat(upper(text), \"!!!\")\n",
    "\n",
    "/*Now select the function*/\n",
    "SELECT yelling(food) FROM foods\n",
    "\n",
    "/*Output is APPLE!!!, BANANA!!!, CARROT!!!*/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Control Flow Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE FUNCTION foods_i_like(food STRING)\n",
    "RETURNS STRING\n",
    "RETURN CASE \n",
    "  WHEN food = \"beans\" THEN \"I love beans\"\n",
    "  WHEN food = \"potatoes\" THEN \"My favorite vegetable is potatoes\"\n",
    "  WHEN food <> \"beef\" THEN concat(\"Do you have any good recipes for \", food ,\"?\")\n",
    "  ELSE concat(\"I don't eat \", food)\n",
    "END;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the location of a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    " DESCRIBE FUNCTION EXTENDED get_url\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Higher Order Functions DE4.7\n",
    "\n",
    "FILTER filters an array using the given lambda function.\n",
    "EXIST tests whether a statement is true for one or more elements in an array.\n",
    "TRANSFORM uses the given lambda function to transform all elements in an array.\n",
    "REDUCE takes two lambda functions to reduce the elements of an array to a single value by merging the elements into a buffer, and the apply a finishing function on the final buffer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUTO LOADER IS BELOW \n",
    "\n",
    "DE 6.0 series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "readStream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#auto loader syntax\n",
    "df_read = (spark.readStream\n",
    "    .format('cloudFiles')\n",
    "    .option('cloudFiles.format', 'csv')\n",
    "    .option('cloudFiles.schemaLocation', {path})\n",
    "    .option('header', 'true')\n",
    "    .load('path')\n",
    "    .schema(schema)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for a table\n",
    "(spark.readStream\n",
    "    .table(\"books\")\n",
    "    .createOrReplaceTempView(\"my_streaming_temp_view\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "writeStream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#auto loader syntax\n",
    "df_read.writeStream(\n",
    "    .format('delta')\n",
    "    .option('cloudFiles.checkpointLocation')\n",
    "    .outputMode('append')\n",
    "    .query('myQuery_name')\n",
    "    .trigger(availableNow=True)\n",
    "    .toTable('my_table_name')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#table syntax\n",
    "(spark.table(\"my_spark_table_vw\")\n",
    "    .writeStream\n",
    "    .trigger(processingTime='4 seconds')\n",
    "    .outputMode(\"complete\") #complete = full re-write. append = incremental load. Aggregations always require the complete mode\n",
    "    .option(\"checkpointLocation\", \"dbfs:/mnt/demo/author_counts_checkpoint\")\n",
    "    .table(\"my_table_name\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
